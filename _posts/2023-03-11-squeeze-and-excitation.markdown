---
title:  "Squeeze-and-Excitation Blocks"
date:   2023-03-11
mathjax: true
categories:
    - blog
tags: 
    - convolution
    - spatial_encodings
    - adaptive_calibration
    - feature_representation
---
# Squeeze-and-Excitation Blocks

[Squeeze and Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)

A unit of a transformation $F_{tr}$ mapping an input $$ X \in \R^{H \times W \times C} $$ to feature maps $$ U \in \R^{H^{'} \times W^{'} \times C^{'}} $$. Taking $$ F_{tr} $$ to be a convolution operator and use $$ V = [v_1, v_2, \cdots, v_{C}] $$ to denote the learned set of filter, kernels, where $v_c$ refers to the parameters of the $c$-th filter. We can then write the outputs $$ U \in \R^{H^{'} \times W^{'} \times C^{'}} $$ as 

$$ u_c = v_c * X = \sum_{s=1}^C v_c^s * x^s $$

where $*$ denotes convolution and $v_c^s$ is a 2D spatial kernel representing a single channel of $v_c$ that acts on the corresponding channel of $X$. Since the output is a summation through all channels, channel dependecies are implicitly embedded in $v_c$, but are entangled with the local spatial correlation captured by the filters. The channel relationships modelled by convolution are inherently implicit and local. 

The learning of convolutional features is expected to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations. Consequently, providing it with access to global information and recalibrate filter responses in  two steps, $\textit{squeeze}$ and $\textit{excitation}$, before they are fed into the next transformation. 


## Squeeze: Global Information Embedding


Each of the learned filters operates with a local receptive field. Consequently each unit of the transformation output $U$ is unable to exploit contextual information outside of this region. "Squeezing" global spatial information into a channel descriptor aims to mitigate this problem by using a global average pooling to generate channel=wise statistics. 

Formally, a statistic $z \in \R^C$ is generated by shrinking $U$ through its spatial dimensions $H \times W$, such that the $c$-th element of $z$ is calculated by:

$$ z_c = F_{sq}(u_c) = \frac{1}{H \times W}\sum_{i=1}^{H}\sum_{j=1}^W u_c(i,j)$$


## Excitation: Adaptive Recalibration

We follow the $\textit{squeeze}$ operation with a second operation which aims to fully cpature channel-wise dependencies. To fulfil this objective, the function must:

* Be capable of learning a nonlinear interaction between channels
* Learn a non-mutually-exclusive relationship to ensure that multiple channels are allowed to be emphasised (rather than enforcing a one-hot activation)

The following matches this criteria:

$$s = F_{ex}(z, W) = \sigma(g(z,W)) = \sigma(W_2 \delta(W_1z))$$

where $\delta$ refers to the ReLU function, $W_1 \in \R^{\frac{C}{r} \times C}$ and $W_2 \in \R^{C \times \frac{C}{r}}$. This gating mechanism is parameterized by forming a bottleneck with two fully-connected layers around the non-linearity i.e. a dimensionality-reduction layer with reduction ratio $r$, a ReLU and then a dimensionality-increasing layer returning to the channel dimension of the transformation output $U$. The final output of the block is obtained by rescaling $U$ with the activations $s$:

$$\tilde{x}_c = F_{scale}(u_c, s_c) = s_c u_c$$


where $\tilde{X} = [\tilde{x_1},\tilde{x_2},\cdots,\tilde{x_c}]$ and $F_{scale}(u_c, s_c)$ refers to the channel-wise multiplication between the scalar $s_c$ and the feature map $u_c \in \R^{H \times W}$.

Note, the excitation operator maps the input-specific descriptor $z$ to a set of channel weights. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a self=attention function on channels whose relationships are not confined to the local receptive field the convolution filters are repsonsive to. 

![alt]({{ site.url }}{{ site.baseurl }}assets/images/squeeze-and-excitation-blocks/seblocks.png)