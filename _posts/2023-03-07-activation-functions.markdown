---
title:  "Searching for Activation Functions"
date:   2023-03-08
categories:
    - study
tags: 
    - ReLU
    - Swish
    - Activation
---

# Searching for Activation Functions

[Searching for Activation Functions](https://arxiv.org/pdf/1710.05941.pdf?ref=paperspace-blog)

ReLU, defined as $f(x) = \max(0, x)$ was a breakthrough that enabled the fully supervised training of state-of-the-art deep neural networks. Deep networks with ReLU are more easily optimized than sigmoid or tanh units, because gradients are able to flow when the input to the ReLU function is positive. Thanks to its simplicity and effectiveness, ReLU has
become the default activation function used across the deep learning community.

In this paper, the authors propose to use automated search techniques to discover novel activation functions. They focus on finding new scalar activation functions, which take in as input a scalar and output a scalar, because scalar activation functions can be used to replace the ReLU function without changing the network architecture. 

## Search Space

An important challenge in designing search spaces is balancing the size and expressivity of the search space. An overly constrained search space will not contain novel activation functions, whereas a search space that is too large will be difficult to effectively search. To balance the two criteria, they design a simple search space that composes unary and binary functions to construct the activation function. Unary functions take in a single scalar input and return a single scalar output such as $u(x) = x^2$ or $u(x) = \sigma(x)$. Binary functions take in two scalar inputs and return a single scalar output such as $b(x_1, x_2) = x_1 \cdot x_2$. The activation function is constructed by repeatedly composing the “core unit” - which takes in two scalar inputs, passes each input independently through an unary function, and combines the two unary outputs with a binary function that outputs a scalar. Given the search space, the goal of the search algorithm is to find effective choices for the unary and binary functions.

## Method

An RNN Controller is used to predict a single component of the activation function.  The prediction is fed back to the controller in the next timestep, and this process is repeated until every
component of the activation function is predicted. The predicted string is then used to construct the activation function. Once a candidate activation function has been generated by the search algorithm, a “child network” with the candidate activation function is trained on some task, such as image classification on CIFAR-10. After training, the validation accuracy of the child network is recorded and used to update the search algorithm. The controller is trained with reinforcement learning to maximize the validation accuracy, where the validation accuracy serves as the reward which pushes the controller to generate activation functions that have high validation accuracies. 

## Search Findings

Using Resnet-20 as the child network on CIFAR-10 image classification tasks, the following findings were made by the RNN Controller trained on PPO, using the exponential moving average of rewards as a baseline to reduce variance:

* Complicated activation functions consistently underperform simpler activation functions, potentially due to an increased difficulty in optimization. The best performing activation functions can be represented by 1 or 2 core units
* Functions that use division tend to perform poorly because the output explodes when the denominator is near 0.
* A common structure shared by the top activation functions is the use of the raw preactivation x as input to the final binary function.

Among the activation functions that outperform ReLU are $x \cdot \sigma(\beta x)$ and $\max(x, \sigma(x))$.

## SWISH

SWISH is defined as $x \cdot \sigma(\beta x)$ where $\sigma(z) = (1 + \exp(-z))^{-1}$ is the sigmoid function and $\beta$ is a constant or trainable parameter. If $\beta = 1$ then SWISH becomes the Sigmoid-weighted Linear Unit (SiL) that was proposed for reinforcement learning. If $\beta = 0$ then SWISH is simply $f(x) = \frac{x}{2}$. As $\beta \rightarrow \infty$ the sigmoid component approaches a $0-1$ function, so Swish becomes like the ReLU function. This suggests that Swish can be loosely viewed as a smooth function which nonlinearly interpolates between the linear function and the ReLU function. The degree of interpolation can be controlled by the model if $\beta$ is set as a trainable parameter.

Like ReLU, Swish is unbounded above and bounded below. Unlike ReLU, Swish is smooth and nonmonotonic. In fact, the non-monotonicity property of Swish distinguishes itself from most common activation functions. 

<figure>
<img src="assets/images/searching-for-activation-functions/swish-activation-first-deriv.png">
</figure>

When $\beta = 1$, the derivative of Swish has magnitude less than 1 for inputs that are less than around 1.25. Thus, the success of Swish with $\beta = 1$ implies that the gradient preserving property of ReLU (i.e. having a derivative of 1 when $x > 0$) may no longer be a distinct advantage in modern architectures. Practically, if Batch Normalization is used, the scale parameter should be set as Swish is not piecewise linear unlike ReLU. 

